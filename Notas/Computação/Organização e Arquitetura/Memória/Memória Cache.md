A **memória cache** é a unidade de armazenamento mais rápida do computador. Isso ocorre devido ao uso da tecnologia SRAM, que é muito mais rápida que a DRAM, utilizada nos demais componentes de armazenamento (exceto os componentes de armazenamento não-volátil). Contudo, essa tecnologia é muito cara, e por isso é utilizada apenas na memória mais próxima do processador, e em pequena quantidade.

# Utilização
A memória cache é um dos componentes mais importantes da arquitetura da [[CPU]]. Para escrever código de forma eficiente, desenvolvedores devem ter um entendimento de como a cache no sistema em uso funciona.

A cache é uma cópia muito rápida do conteúdo da memória principal do sistema, que é mais lenta. A cache é também muito menor que a memória principal, pois tem que ser embarcada no chip do processador juntamente com [[Registradores|registradores]] e o [[Datapath|caminho de dados]]. Isso é uma localização de grande vantagem em termos de computação, e tem tanto limitações econômicas quanto físicas para o seu tamanho máximo. Enquanto os fabricantes encontram mais formas de colocar mais transistores em um chip, o tamanho das caches aumentam, mas mesmo assim as maiores caches são de dezenas de megabytes, em comparação com os gigabytes da memória principal ou os terabytes dos discos rígidos.

A cache é composta de pequenos pedaços de memória principal espelhada. O tamanho desses pedaços é chamado *line size* (tamanho de linha), e é tipicamente algo como 32 ou 64 bytes. Quando falando sobre cache, é bastante comum falar sobre o line size, ou cache line, que se refere a um dos pedaços da memória principal espelhada. A cache só pode armazenar e acessar memória em tamanhos múltiplos de uma cache line.

As caches também costumam ter sua própria hierarquia, frequentemente chamadas L1, L2 e L3. sendo L1 a menor e mais rápida, e a L3 a maior e mais lenta. A cache L1 ainda costuma ser dividida entre cache de intruções e dados, no que é conhecida como *Arquitetura de Harvard*. Essa divisão ajuda a reduzir gargalos no [[Pipeline|pipeline]] uma vez que os estágios anteriores do pipeline tendem a referenciar a cache de instruções, enquanto os posteriores tendem a referenciar a cache de dados.

# Associatividade da cache
Durante operação normal, o processador está constantemente pedindo à cache que verifique se um endereço particular está armazenado na cache, então a cache precisa de alguma forma de encontrar rapidamente se há uma linha válida ou não. Se um endereço pode ser armazenado em qualquer lugar na cache, toda a cache precisa ser buscada todas as vezes em que é feita uma referência, para verificar se é um **hit** ou **miss**. Para garantir a velocidade do sistema, isso é realizado em paralelo no hardware, mas buscar todas as posições geralmente acarreta em uma implementação muito cara para uma cache de tamanho razoável. Assim, a cache pode se tornar mais simples se criamos regras para onde cada endereço deve ser repassado. Isso é uma troca; o cache é muito menor que a memória principal, então há diferentes posições na memória principal compartilhando um mesmo endereço na cache. Se dois endereços que compartilham a posição na cache estão sendo atualizados constantemente, dizemos que eles estão **disputando a linha da cache** (***fighting over the cache line***). Dessa forma, podemos categorizar as caches em três grupos:
- Caches de **Mapeamento Direto** permitem que uma linha da cache exista em apenas uma entrada da cache. Essa é a forma mais simples de implementar em hardware, mas não evita o compartilhamento de endereço.
- Caches de **Associação Total** permitem que a linha de cache exista em qualquer entrada da cache. Isso evita o compartilhamento, já que qualquer entrada está disponível para uso. Contudo, é muito caro implementar em hardware pois todas as posições possíveis devem ser verificadas simultaneamente para determinar se um valor está na cache.
- Caches de **Associação Conjunta** são um híbrido das duas anteriores, permitindo um valor particular da cache existir em algum subconjunto das linhas dentro da cache. A cache é dividida em compartimentos iguais chamados **vias** (***ways***), e um endereço específico pode estar em qualquer via. Assim, uma cache de associação conjunta de $n$ vias permite que uma linha da cache exista em qualquer posição de um total de blocos $\text{mod}\space n$. Quanto maior o número de vias, mais possibilidades existem para cada linha da cache, resultando em menos compartilhamentos e uma performace melhor.

Quando a cache está cheia, o processador precisa de livrar de uma linha para liberar espaço para uma nova. Existem diversos [[Algoritmos|algoritmos]] pelos quais o processador pode escolher qual linha liberar; por exemplo o _least recently used (LRU)_, que discarta a linha que está a mais tempo sem ser utilizada.
Quando só se lê a cache, não há necessidade de garantir consistência com a memória principal. Contudo, quando o processador escreve nas linhas da cache, é preciso tomar algumas decisões sobre como atualizar as posições referenciadas da memória principal.
